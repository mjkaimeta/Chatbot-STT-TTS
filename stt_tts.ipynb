{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Speech & Speech-to-Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Speech\n",
    "\n",
    "- Video: https://www.youtube.com/watch?v=iwVaAAEE4fo\n",
    "- Implementation: https://github.com/Jalsemgeest/Python/blob/main/VirtualAssistant/assistant.py\n",
    "\n",
    "- Chainging assistant voice: https://gtts.readthedocs.io/en/latest/module.html#localized-accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install playsound\n",
    "import playsound # for playing the audio\n",
    "import speech_recognition as sr # for STT\n",
    "from gtts import gTTS # for TTS (creates the audio file)\n",
    "import os # to delete the audio file\n",
    "import time # checking how long it takes each step\n",
    "from tqdm import tqdm\n",
    "\n",
    "# !pip install einops -q # is a dependancy for microsoft/phi-2 model\n",
    "# !pip install accelerate -q # needed to use low_cpu_mem_usage for HF models\n",
    "# follow these steps to use accelerate https://huggingface.co/docs/accelerate/basic_tutorials/install\n",
    "\n",
    "# def speech_to_text(time_: bool = True):\n",
    "#     recognizer = sr.Recognizer()\n",
    "\n",
    "#     with sr.Microphone() as source:\n",
    "#         # print(\"Listening...\")\n",
    "#         recognizer.adjust_for_ambient_noise(source)\n",
    "#         audio = recognizer.listen(source, timeout = 10) # 10s timeout\n",
    "\n",
    "#     try:\n",
    "#         if time_:\n",
    "#             start = time.time()\n",
    "#         command = recognizer.recognize_google(audio)\n",
    "#         if time_:\n",
    "#             end = time.time() - start\n",
    "#         if time_:\n",
    "#             return command.lower(), end\n",
    "#         return command.lower()\n",
    "    \n",
    "#     except sr.UnknownValueError:\n",
    "#         print(\"Could not understand audio. Please try again.\")\n",
    "#         # return None\n",
    "#     except sr.RequestError:\n",
    "#         print(\"Unable to access the Google Speech Recognition API.\")\n",
    "#         # return None\n",
    "\n",
    "# will keep looping until it work\n",
    "def speech_to_text(time_: bool = True):\n",
    "    recognizer = sr.Recognizer()\n",
    "    try_again_text = \"Sorry, I couldn't understand. Please say that again.\"\n",
    "    while True:\n",
    "        with sr.Microphone() as source:\n",
    "            # print(\"Listening...\")\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            audio = recognizer.listen(source, timeout = 10) # 10s timeout\n",
    "        try:\n",
    "            if time_:\n",
    "                start = time.time()\n",
    "            command = recognizer.recognize_google(audio)\n",
    "            if time_:\n",
    "                end = time.time() - start\n",
    "            if time_:\n",
    "                return command.lower(), end\n",
    "            return command.lower()\n",
    "        \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio. Please try again.\")\n",
    "            text_to_speech(try_again_text)\n",
    "            continue\n",
    "        except sr.RequestError:\n",
    "            print(\"Unable to access the Google Speech Recognition API.\")\n",
    "            text_to_speech(try_again_text)\n",
    "            continue\n",
    "\n",
    "def text_to_speech(response_text):\n",
    "    # print(response_text)\n",
    "    file_name = \"response.mp3\"\n",
    "    tts = gTTS(text=response_text, lang='en', tld='co.za') # made the voice UK English\n",
    "    tts.save(file_name)\n",
    "    playsound.playsound(file_name, True) \n",
    "    os.remove(file_name) # delete the file after it's been created\n",
    "\n",
    "def main():\n",
    "\n",
    "    while True:\n",
    "        # Get voice input from user\n",
    "        user_turn, listen_time = speech_to_text(time_ = True) # return time to listen\n",
    "\n",
    "        # loop break condition\n",
    "        if user_turn.lower().strip() == \"exit\":\n",
    "            break\n",
    "\n",
    "        print(f'You said: \"{user_turn}\"')\n",
    "        print(f\"\\n-- {round(listen_time, 2)}s to listen --\\n\") # rounded to 2 decimals\n",
    "\n",
    "        # generate bot response\n",
    "        bot_response = \"I'm pretty confused, hbu\" # default response\n",
    "        text_to_speech(bot_response)\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maurinjoneskai/anaconda3/envs/jarvis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "# !pip install transformers -q\n",
    "from transformers import pipeline, Conversation\n",
    "summarizer = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\") # https://huggingface.co/knkarthick/MEETING_SUMMARY\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# pipe = pipeline(\"conversational\", model = \"microsoft/DialoGPT-small\") # model?\n",
    "pipe = pipeline(\"conversational\", model = \"facebook/blenderbot_small-90M\") # pretty normal but depressing\n",
    "# pipe = pipeline(\"conversational\", model = \"kavindu999/BetterEnglishGPT-v2\") # pretty good - really likes to talk about dog poop\n",
    "# pipe = pipeline(\"conversational\", model = \"DarrenLo/fine-tuned-dialogpt-pal\") # horrendus. doesn't even resemble human language\n",
    "\n",
    "def get_model_inference(user_text: str):\n",
    "    # user_text = \"what's going on\"\n",
    "    convo1 = Conversation(user_text)\n",
    "    # return pipe([convo1]) # str of model output\n",
    "    return type(pipe([convo1])) # str of model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"conversational\", model = \"facebook/blenderbot_small-90M\")\n",
    "def get_model_inference(user_text: str):\n",
    "    \"\"\"\n",
    "    This function uses the Hugging Face's pipeline for conversational models to generate a response to the \n",
    "    provided user text. The model used is Facebook's Blenderbot.\n",
    "    Parameters:\n",
    "    user_text (str): The text input from the user to which the model will generate a response.\n",
    "    Returns:\n",
    "    str: The generated response from the model to the user's text. If an error occurs during the model inference, \n",
    "    it returns a string with the error message.\n",
    "    Raises:\n",
    "    Exception: If there's an error during the model inference.\n",
    "    \"\"\"\n",
    "    convo = Conversation(user_text)\n",
    "\n",
    "    # if there's model inference\n",
    "    try:\n",
    "        return pipe([convo])[-1]['content'] # str of most recent model message\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "# user_text = \"what time is it\"\n",
    "# get_model_inference(user_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(str_lst: list, chunk_size: int = 5000):\n",
    "    #### summarizer caps out around 5500 chars, so setting len(chunk) max to 5500 ####\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for idx, string in enumerate(str_lst):\n",
    "        if (len(chunk) + len(string)) < chunk_size: # if within range, add to current chunk\n",
    "            chunk += string + \"\\n\"\n",
    "        else: # else create a new chunk and start adding to that one\n",
    "            chunks.append(chunk)\n",
    "            chunk = string\n",
    "\n",
    "        if chunk and (idx+1 == len(str_lst)): # if chunk is not empty and is last chunk in list\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Turns: [\"i don't know i'm just looking for someone to talk to\", 'i really want to tell someone about my breakfast', 'i had scrambled eggs a bagel and some toast and it was just really good today', \"you've never had a bagel\", 'why are you not a fan of bagels', \"well yeah they're not teeth they're bagels\", 'have you ever eaten teeth', \"really eating teeth is good for your teeth i didn't know that\", 'how many teeth do you usually eat on a daily basis', \"that's important fruits and veggies are really good for your health probably better than teeth\", \"so you don't think fruits are better for you than teeth\", 'better for you than what', 'how is that related to eating teeth', 'well i guess that makes sense it would be difficult to eat your own teeth with your teeth', 'yes that sounds bad okay', \"i would recommend you don't eat your own teeth in a relationship\", 'that is a very important fundamental value to share', \"who's doing what they're doing\", \"aren't we all what are you think you'll do with your life\", \"that's fair i don't think anyone really knows\", 'that feeling can be very isolating', \"that's not a good feeling\", 'what are you doing now', 'what is the project', 'what kind of law would you like to work in', 'if you had to pick one kind of law what would it be', 'that makes sense it seems like a big decision that will affect you for a long time', 'stop telling me about your relationship', \"i don't want to tell you anything about my relationship what do you want to tell me about your relationship\", \"how often do you eat each other's teeth\", 'that sounds like a healthy number of times per week', \"okay i'm going to go now\", \"i mean that i'm leaving now\"]\n",
      "Model Turns: [\"i'm sorry to hear that. what are you looking for in someone to talk to?\", 'what kind of breakfast is it? what is it that you want to tell someone about it?', \"that sounds yummy. i've never had a bagel before. what did you put in it?\", \"i've never had a bagel. i'm not a fan of bagels.\", \"i'm not a fan of bagels. they're just not my cup of tea.\", \"that's a good point. i've never had bagels before, so i didn't think of that.\", \"no i haven't, but i've heard it's good for your teeth.\", \"yeah, it's really good for your teeth, especially if you eat a lot of it.\", \"i don't eat too many teeth, but i do eat a lot of fruits and vegetables.\", \"i don't think so. i've been eating a lot of fruits and veggies lately.\", \"i don't think fruits are better for me than teeth, but i do think they're better for you.\", \"i think it's better for me because i don't have to worry about getting hit by a car.\", \"i'm not sure. i guess it's because i've never been able to eat my own teeth.\", \"yeah, but it's not as bad as eating your own teeth with your own tongue.\", \"it's been a long time since i've been in a relationship. i don't know what to do.\", \"that's a good idea. i'm not sure if i want to be in a relationship with someone who doesn't eat their own teeth.\", \"it really is. i'm so proud of him for doing what he's doing.\", \"i don't know. i'm trying to figure out what to do with my life.\", \"i don't know. i'm not sure what i want to do with my life.\", \"yeah, i don't think anyone really knows me either. i guess i'll just have to deal with it.\", \"it really is. i can't help but feel like i'm going to die.\", \"it's not. i'm going to have to find a new job. i don't know what to do.\", \"i'm working on a new project. it's going to be a lot of fun.\", \"i'm trying to get a job at a law firm. i want to be a lawyer.\", \"i don't know yet. i'm not sure what i want to do yet.\", \"i don't know... i guess i'd have to think about it.\", \"yeah, it's been a long time since i've been in a relationship. i don't know what i'm going to do.\", \"i'm sorry. what do you want to tell me about your relationship with your partner?\", \"i don't want to tell you anything about my relationship. what do you want to know?\", \"we eat each other's teeth at least once a week. it's so gross.\", \"yeah, it's a lot of work, but i'm happy to do it.\", \"what are you going to do? i'm sure you'll have a great time.\", \"i'm sorry to hear that. what are you going to do now that you're gone?\"]\n"
     ]
    }
   ],
   "source": [
    "test_convo = open('data/logged_convos/20231228135643/20231228135643_convo.txt', \"r\").read().strip()\n",
    "all_turns = [line[line.find(\"USER:\"): ].strip() for line in test_convo.split(\"\\n\\n\") if line[line.find(\"USER:\"): ].strip()]\n",
    "\n",
    "user_turns = []\n",
    "model_turns = []\n",
    "# Loop through the list\n",
    "for turn in all_turns:\n",
    "    if not turn.strip():\n",
    "        continue\n",
    "    # Split the turn by 'MODEL:'\n",
    "    split_turn = turn.split('MODEL:')\n",
    "    \n",
    "    # Add the user turn to the user_turns list\n",
    "    user_turns.append(split_turn[0].strip().replace('USER: ', ''))\n",
    "    \n",
    "    # Add the model turn to the model_turns list\n",
    "    model_turns.append(split_turn[1].strip())\n",
    "# Print the lists\n",
    "print(\"User Turns:\", user_turns)\n",
    "print(\"Model Turns:\", model_turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize_convo(user_turns, model_turns):\n",
    "#     # assert len(history) > 1, (\"There must be at least one user turn and one model turn of conversation to summarize\") # must be at least one full turn\n",
    "#     # uses https://huggingface.co/knkarthick/MEETING_SUMMARY\n",
    "\n",
    "#     print(\"Summarizing conversation data...\")\n",
    "\n",
    "#     max_inp_toks = summarizer.tokenizer.max_len_single_sentence\n",
    "#     inp_cutoff = max_inp_toks - (max_inp_toks//10) # 10% less than max toks just to be sure\n",
    "\n",
    "#     user_turns = \"\\n- \".join(user_turns)\n",
    "#     if count_tokens(user_turns) > inp_cutoff:\n",
    "#         user_sum = \"\"\n",
    "#         chunks = chunk_turns(user_turns, inp_cutoff)\n",
    "#         for chunk in tqdm(chunks, desc = \"User chunk:\"):\n",
    "#             interm_summ = summarizer(chunk)[0]['summary_text'].strip()\n",
    "#             user_sum += interm_summ + \"\\n\"\n",
    "#     else:\n",
    "#         user_sum = summarizer(user_turns)[0]['summary_text'].strip()\n",
    "#     print(\"\\tSummarized 'USER' turns\")\n",
    "    \n",
    "#     model_turns = \"\\n- \".join(model_turns)\n",
    "#     # model_sum = summarizer(model_turns)[0]['summary_text'].strip() # joining by \"\\n- \" because it's more similar to the finetuning data\n",
    "#     if count_tokens(model_turns) > inp_cutoff:\n",
    "#         model_sum = \"\"\n",
    "#         chunks = chunk_turns(model_turns, inp_cutoff)\n",
    "#         for chunk in tqdm(chunks, desc = \"Model chunk:\"):\n",
    "#             interm_summ = summarizer(chunk)[0]['summary_text'].strip()\n",
    "#             model_sum += interm_summ + \"\\n\"\n",
    "#     else:\n",
    "#         model_sum = summarizer(model_turns)[0]['summary_text'].strip()\n",
    "#     print(\"\\tSummarized 'MODEL' turns\")\n",
    "    \n",
    "#     # each turn concatenated together, prefixed by \"- \" and \"\\n\" appended, to make closely mimic the finetuning data of the summarization model\n",
    "#     # all_turns = \"\".join(\"- \" + turn['user'] + \"\\n- \" + turn['model'] + \"\\n\" for turn in history).strip()\n",
    "#     # all_turns = (\"- \" + turn['user'] + \"\\n- \" + turn['model'] + \"\\n\" for turn in history).strip()\n",
    "#     all_turns = (user_turns + model_turns)\n",
    "#     if count_tokens(all_turns) > inp_cutoff:\n",
    "#         total_sum = \"\"\n",
    "#         chunks = chunk_turns(all_turns, inp_cutoff)\n",
    "#         for chunk in tqdm(chunks, desc = \"Combined chunks:\"):\n",
    "#             interm_summ = summarizer(chunk)[0]['summary_text'].strip()\n",
    "#             total_sum += interm_summ + \"\\n\"\n",
    "#     else:\n",
    "#         total_sum = summarizer(all_turns)[0]['summary_text'].strip()\n",
    "#     print(\"\\tSummarized 'USER' and 'MODEL' turns concatenated chronologically\")\n",
    "#     print(\"...summaries completed.\\n\")\n",
    "\n",
    "#     return user_sum.strip(), model_sum.strip(), total_sum.strip()\n",
    "\n",
    "# summarize_convo(user_turns, model_turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_datetime():\n",
    "    \"\"\"\n",
    "    This function returns the current date and time as a string in the format 'YYYYMMDDHHMMSS'.\n",
    "    \n",
    "    Returns:\n",
    "        str: The current date and time.\n",
    "    \"\"\"\n",
    "    return datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "def count_tokens(text: str, hf_tokenizer = summarizer.tokenizer, warning: bool = True):\n",
    "    toks = len(hf_tokenizer.tokenize(text))\n",
    "    max_toks = summarizer.tokenizer.max_len_single_sentence\n",
    "    sugg_max = max_toks - (max_toks//10)\n",
    "    if warning:\n",
    "        if toks > sugg_max:\n",
    "            print(f\"WARNING: Max input tokens for this model is {max_toks}. Suggested max input is {sugg_max} tokens, while this string is currently {toks} tokens.\")\n",
    "    return toks\n",
    "\n",
    "def chunk_turns(turns_str: str, max_length: int):\n",
    "    # Split the string by \"\\n\"\n",
    "    split_str = turns_str.split(\"\\n\")\n",
    "    \n",
    "    # Initialize the chunks_list and current_chunk\n",
    "    chunks_list = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    # Iterate over each split section\n",
    "    for section in split_str:\n",
    "        if current_length + count_tokens(section) > max_length:\n",
    "            # If adding the next section exceeds the max_length, add the current chunk to chunks_list\n",
    "            chunks_list.append(\"\\n\".join(current_chunk))\n",
    "            # Start a new chunk with the current section\n",
    "            current_chunk = [section]\n",
    "            current_length = len(section)\n",
    "        else:\n",
    "            # If adding the next section doesn't exceed the max_length, add the section to the current chunk\n",
    "            current_chunk.append(section)\n",
    "            current_length += len(section)\n",
    "    \n",
    "    # Add the last chunk to chunks_list if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks_list.append(\"\\n\".join(current_chunk))\n",
    "    \n",
    "    # Return the final list\n",
    "    return chunks_list\n",
    "\n",
    "class Convo():\n",
    "    \"\"\"\n",
    "    This class represents a conversation with a conversational model. It logs the turns of the conversation, \n",
    "    the user's and model's inputs, and optionally the inference times. It also provides methods to get the \n",
    "    conversation as a DataFrame and to create a log of the conversation.\n",
    "    \n",
    "    Attributes:\n",
    "        history (list): A list of dictionaries representing the conversation log.\n",
    "        model_name (str): The name of the conversational model.\n",
    "        turn (int): The current turn of the conversation.\n",
    "        inference_times (list): A list of dictionaries representing the inference times for each turn.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = None):\n",
    "        \"\"\"\n",
    "        The constructor for the Convo class. Initializes the history, model_name, turn, and inference_times \n",
    "        attributes.\n",
    "\n",
    "        Parameters:\n",
    "            model_name (str): The name of the conversational model. Default is None.\n",
    "        \"\"\"\n",
    "        self.history = [] # conversation log -> [{\"turn\": x, \"user\": \"asdf\", \"model\": \"asdf\"}, ...]\n",
    "        self.model_name = \"No model passed\" if not model_name else model_name\n",
    "        self.turn = 1\n",
    "        self.inference_times = [] # [{\"turn\": int, \"time\": float_seconds}, ...] # optional\n",
    "\n",
    "    def log_turn(self, user_content: str, model_content: str):\n",
    "        \"\"\"\n",
    "        Logs a turn of the conversation, including the user's and model's inputs.\n",
    "        Parameters:\n",
    "        user_content (str): The user's input for this turn.\n",
    "        model_content (str): The model's input for this turn.\n",
    "        \"\"\"\n",
    "        self.history.append(\n",
    "            {\n",
    "                \"turn\": self.turn,\n",
    "                \"user\" : user_content,\n",
    "                \"model\" : model_content\n",
    "            }\n",
    "        )\n",
    "        self.turn += 1 # increment turn\n",
    "\n",
    "    # def summarize_convo(self):\n",
    "    #     assert len(self.history) > 1, (\"There must be at least one user turn and one model turn of conversation to summarize\") # must be at least one full turn\n",
    "    #     # uses https://huggingface.co/knkarthick/MEETING_SUMMARY\n",
    "\n",
    "    #     print(\"Summarizing conversation data...\")\n",
    "\n",
    "    #     max_inp_toks = summarizer.tokenizer.max_len_single_sentence\n",
    "    #     inp_cutoff = max_inp_toks - (max_inp_toks//10) # 10% less than max toks just to be sure\n",
    "\n",
    "    #     user_turns = \"\\n- \".join([turn['user'] for turn in self.history])\n",
    "    #     if count_tokens(user_turns) > inp_cutoff:\n",
    "    #         user_sum = \"\"\n",
    "    #         chunks = chunk_turns(user_turns, inp_cutoff)\n",
    "    #         for chunk in tqdm(chunks, desc = \"User chunk:\"):\n",
    "    #             interm_summ = summarizer(chunk)[0]['summary_text'].strip()\n",
    "    #             user_sum += interm_summ + \"\\n\"\n",
    "    #     else:\n",
    "    #         user_sum = summarizer(user_turns)[0]['summary_text'].strip()\n",
    "    #     print(\"\\tSummarized 'USER' turns\")\n",
    "        \n",
    "    #     model_turns = \"\\n- \".join([turn['model'] for turn in self.history])\n",
    "    #     # model_sum = summarizer(model_turns)[0]['summary_text'].strip() # joining by \"\\n- \" because it's more similar to the finetuning data\n",
    "    #     if count_tokens(model_turns) > inp_cutoff:\n",
    "    #         model_sum = \"\"\n",
    "    #         chunks = chunk_turns(model_turns, inp_cutoff)\n",
    "    #         for chunk in tqdm(chunks, desc = \"Model chunk:\"):\n",
    "    #             interm_summ = summarizer(chunk)[0]['summary_text'].strip()\n",
    "    #             model_sum += interm_summ + \"\\n\"\n",
    "    #     else:\n",
    "    #         model_sum = summarizer(model_turns)[0]['summary_text'].strip()\n",
    "    #     print(\"\\tSummarized 'MODEL' turns\")\n",
    "        \n",
    "    #     # each turn concatenated together, prefixed by \"- \" and \"\\n\" appended, to make closely mimic the finetuning data of the summarization model\n",
    "    #     # all_turns = \"\".join(\"- \" + turn['user'] + \"\\n- \" + turn['model'] + \"\\n\" for turn in self.history).strip()\n",
    "    #     all_turns = (\"- \" + turn['user'] + \"\\n- \" + turn['model'] + \"\\n\" for turn in self.history).strip()\n",
    "    #     if count_tokens(all_turns) > inp_cutoff:\n",
    "    #         total_sum = \"\"\n",
    "    #         chunks = chunk_turns(all_turns, inp_cutoff)\n",
    "    #         for chunk in tqdm(chunks, desc = \"Combined chunks:\"):\n",
    "    #             interm_summ = summarizer(chunk)[0]['summary_text'].strip()\n",
    "    #             total_sum += interm_summ + \"\\n\"\n",
    "    #     else:\n",
    "    #         total_sum = summarizer(all_turns)[0]['summary_text'].strip()\n",
    "    #     print(\"\\tSummarized 'USER' and 'MODEL' turns concatenated chronologically\")\n",
    "    #     print(\"...summaries completed.\\n\")\n",
    "\n",
    "    #     return user_sum, model_sum, total_sum\n",
    "    \n",
    "    def get_convo_df(self):\n",
    "        \"\"\"\n",
    "        Converts the conversation log into a DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: A DataFrame representing the conversation log.\n",
    "        \n",
    "        Raises:\n",
    "            AssertionError: If there is not at least one full turn of conversation.\n",
    "        \"\"\"\n",
    "        assert len(self.history) > 1, (\"There must be at least one user turn and one model turn of conversation to create a DataFrame\") # must be at least one full turn\n",
    "\n",
    "        df_dict = {key: [] for key, _ in self.history[0].items()} # setup empty dict with same keys as history and lists\n",
    "        for turn in self.history:\n",
    "            for key, val in turn.items():\n",
    "                df_dict[key].append(val)\n",
    "\n",
    "        return pd.DataFrame(df_dict)\n",
    "\n",
    "    def create_convo_log(self):\n",
    "        \"\"\"\n",
    "        Creates a log of the conversation, saving it as both a .csv and .txt file in a folder named with the \n",
    "        current date and time.\n",
    "        \n",
    "        Raises:\n",
    "            OSError: If the function is unable to create the folder or the files.\n",
    "        \"\"\"\n",
    "        # create convo folder\n",
    "        data_dir = \"data/logged_convos/\"\n",
    "        datetime = get_current_datetime()\n",
    "        convo_folder = f\"{data_dir + datetime}\"\n",
    "\n",
    "        if not os.path.exists(convo_folder):\n",
    "            os.makedirs(convo_folder)\n",
    "        \n",
    "        base_file_name = f\"{convo_folder}/{datetime}_convo\"\n",
    "\n",
    "        ### log convo in different formats\n",
    "        # .csv\n",
    "        convo_df = self.get_convo_df() #### CHANGE THIS\n",
    "        convo_df.to_csv(f\"{base_file_name}.csv\", index = False) # 'turn' col functions as idx\n",
    "\n",
    "        # .txt\n",
    "        with open(f\"{base_file_name}.txt\", \"w\") as f:\n",
    "            for turn_log in self.history:\n",
    "                f.write(\n",
    "                    f\"Turn: {turn_log['turn']}\\n\\tUSER: {turn_log['user']}\\n\\tMODEL: {turn_log['model']}\\n\\n\" # can be split by (\"\\n\\n\") later to get turn-by-turn\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[LISTENING]]\n",
      "USER: \"how are you doing today\"\n",
      "BOT: \"i'm doing well, thank you. what about you? what are you up to?\"\n",
      "\n",
      "[[LISTENING]]\n",
      "USER: \"nothing much just working away\"\n",
      "BOT: \"i'm sorry to hear that. i hope you're able to get back on your feet soon.\"\n",
      "\n",
      "[[LISTENING]]\n",
      "USER: \"thank you i'm going to leave now\"\n",
      "BOT: \"i'm sorry to hear that. i hope you're able to find a good home.\"\n",
      "\n",
      "[[LISTENING]]\n",
      "USER: \"thanks i have to go\"\n",
      "BOT: \"you're welcome. i'm sure you'll have a great time. what are you going to do?\"\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Convo at 0x136e7b290>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main(show_timing: bool = False):\n",
    "\n",
    "    # opening message\n",
    "    intro = \"Hello. How can I be of assistance?\"\n",
    "    text_to_speech(intro)\n",
    "\n",
    "    convo = Convo()\n",
    "\n",
    "    while True:    \n",
    "\n",
    "        print(\"\\n[[LISTENING]]\\n\\n\") # to get overwritten by other text\n",
    "\n",
    "        # Get voice input from user\n",
    "        user_turn, listen_time = speech_to_text(time_ = True) # return time to listen\n",
    "\n",
    "        # loop break condition\n",
    "        break_conds = [\n",
    "            \"time to go\", \"exit\", \"I'm leaving\", \"bye\", \"i need to leave\", \"blep\",\n",
    "            \"i have to go\"\n",
    "        ]\n",
    "\n",
    "        # checks if break condition keywords are present in transcription\n",
    "        break_now = any(brk_cnd.lower().strip() in user_turn.lower().strip() for brk_cnd in break_conds)\n",
    "\n",
    "        print(f'USER: \"{user_turn}\"')\n",
    "        if show_timing:\n",
    "            print(f\"\\t\\n-- {round(listen_time, 2)}s to listen --\\n\\n\") # rounded to 2 decimals\n",
    "\n",
    "        # model inference (no context)  \n",
    "        inference_start = time.time()\n",
    "        # model_turn = \"\"\n",
    "        # if len(convo.history) > 0:\n",
    "        #     model_turn = convo.history[-1]['model'] # last model turn\n",
    "        # user_turn_w_context = f\"CONTEXT: '{model_turn}'.\\nI SAID: '{user_turn}'\"\n",
    "        # user_turn_w_context = f\"'{model_turn}'.\\n'{user_turn}'\"\n",
    "\n",
    "        # print(f\"Context: {user_turn_w_context}\")\n",
    "\n",
    "        model_turn = get_model_inference(user_turn)\n",
    "        inference_end = time.time() - inference_start\n",
    "\n",
    "        # generate bot response\n",
    "        text_to_speech(model_turn)\n",
    "        \n",
    "        print(f'BOT: \"{model_turn}\"')\n",
    "        if show_timing:\n",
    "            print(f\"\\t\\n-- {round(inference_end, 2)}s to get inference --\\n\") # rounded to 2 decimals\n",
    "        else:\n",
    "            print()\n",
    "\n",
    "        convo.log_turn(\n",
    "            user_content = user_turn,\n",
    "            model_content = model_turn\n",
    "            )\n",
    "\n",
    "        if break_now:\n",
    "            # write convo log to drive\n",
    "            convo.create_convo_log()\n",
    "\n",
    "            return convo\n",
    "            # break\n",
    "\n",
    "convo_log = main(show_timing = False, print_newlines = False)\n",
    "convo_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn</th>\n",
       "      <th>user</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>how are you doing today</td>\n",
       "      <td>i'm doing well, thank you. what about you? wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>nothing much just working away</td>\n",
       "      <td>i'm sorry to hear that. i hope you're able to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>thank you i'm going to leave now</td>\n",
       "      <td>i'm sorry to hear that. i hope you're able to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>thanks i have to go</td>\n",
       "      <td>you're welcome. i'm sure you'll have a great t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   turn                              user  \\\n",
       "0     1           how are you doing today   \n",
       "1     2    nothing much just working away   \n",
       "2     3  thank you i'm going to leave now   \n",
       "3     4               thanks i have to go   \n",
       "\n",
       "                                               model  \n",
       "0  i'm doing well, thank you. what about you? wha...  \n",
       "1  i'm sorry to hear that. i hope you're able to ...  \n",
       "2  i'm sorry to hear that. i hope you're able to ...  \n",
       "3  you're welcome. i'm sure you'll have a great t...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_log.get_convo_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors.index.json: 100%|██████████| 24.3k/24.3k [00:00<00:00, 66.4MB/s]\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 4.98G/4.98G [02:06<00:00, 39.2MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 577M/577M [00:13<00:00, 44.0MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [02:20<00:00, 70.48s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.71s/it]\n",
      "generation_config.json: 100%|██████████| 69.0/69.0 [00:00<00:00, 185kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 7.34k/7.34k [00:00<00:00, 19.8MB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 2.93MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 12.0MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 17.3MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.08k/1.08k [00:00<00:00, 6.79MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 195kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe_ = pipeline(\"text-generation\", model=\"microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.54s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# code amended from https://huggingface.co/spaces/MISTP/Phi2_Chatbot/blob/main/app.py\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_path = \"finetuned_phi2\"\n",
    "model_path = \"microsoft/phi-2\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maurinjoneskai/anaconda3/envs/jarvis/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou are a question answering chatbot. Provide a clear and detailed explanation\\n<</SYS>>\\n\\n how long until dinner? [/INST]\\nYou are a question answering chatbot. Provide a clear and detailed explanation\\n<</SYS>>\\n\\n how long until dinner? [/INST]\\nYou are a question answering chatbot. Provide a clear and detailed explanation\\n<</SYS>>\\n\\n how long until dinner? [/INST]\\nYou are a question answering chatbot. Provide a clear and detailed explanation\\n<</SYS>>\\n\\n how long until dinner? [/INST]\\nYou are a question answering chatbot. Provide a clear and detailed explanation\\n<</SYS>>\\n\\n how long until dinner? [/INST]\\nYou are a question answering chatbot. Provide a clear and detailed explanation\\n<</SYS>>\\n\\n how long until dinner? [/INST]\\nYou are a question answering chatbot. Provide a clear and detailed explanation\\n<</SYS'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def generate(question, context, max_new_tokens = 200, temperature = 0.6):\n",
    "def generate(question, max_new_tokens = 200, temperature = 0.6):\n",
    "  \n",
    "    system_message = \"You are a question answering chatbot. Provide a clear and detailed explanation\"\n",
    "    prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n {question} [/INST]\" # replace the command here with something relevant to your task\n",
    "\n",
    "    # Count the number of tokens in the prompt\n",
    "    num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n",
    "    # Calculate the maximum length for the generation\n",
    "    max_length = num_prompt_tokens + max_new_tokens\n",
    "\n",
    "    gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length, temperature=temperature)\n",
    "    result = gen(prompt)\n",
    "    return (result[0]['generated_text'].replace(prompt, ''))\n",
    "\n",
    "num_new_tokens = 200  # change to the number of new tokens you want to generate\n",
    "prompt = \"how long until dinner?\"\n",
    "start = time.time()\n",
    "resp = generate(prompt)\n",
    "inf_time = round(time.time() - start, 2)\n",
    "print(inf_time)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performances (Naturalness?)\n",
    "\n",
    "- Have a synethic dataset, created by MetaMate GPT4 of 40 turns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas -q\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/google/Synthetic-Persona-Chat # test set\n",
    "test_set = pd.read_csv(\"data/convo_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[371, 481, 493, 292, 427, 233, 458, 6, 420, 875, 674, 729, 266, 244, 651, 228, 11, 304, 310, 835, 344, 683, 146, 762, 617, 318, 23, 809, 226, 618, 260, 21, 915, 158, 828, 621, 684, 647, 29, 476, 468, 614, 642, 721, 304, 230, 802, 318, 371, 265, 431, 807, 89, 357, 507, 434, 530, 659, 177, 577, 301, 593, 46, 290, 86, 853, 810, 7, 536, 383, 241, 502, 159, 317, 312, 325, 472, 465, 68, 169, 717, 494, 746, 871, 14, 456, 863, 886, 504, 15, 941, 861, 487, 716, 815, 126, 466, 853, 628, 81]\n"
     ]
    }
   ],
   "source": [
    "# random.seed(16) # for reproducibility\n",
    "# random_convos = [random.randint(1, len(test_set)) for _ in range(100)]\n",
    "# print(random_convos)\n",
    "\n",
    "# for i_conv in random_convos:\n",
    "#     convo_turns = \n",
    "#     conv_len = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Hi, I'm [User 1's name]. What's your name?\",\n",
       "  \"Hi, I'm [User 2's name]. It's nice to meet you.\"),\n",
       " ('Nice to meet you too. What are you interested in?',\n",
       "  'I like to meet new people, play ultimate frisbee, and spend time with my family.'),\n",
       " ('Those are all great things to be interested in. I like to dance at the club, run a dog obedience school, and eat a lot of sweets.',\n",
       "  \"That's interesting. I've never met anyone who runs a dog obedience school before.\"),\n",
       " (\"It's a lot of fun. I get to work with dogs every day and help them learn how to behave.\",\n",
       "  'That sounds like a great job. I love dogs.'),\n",
       " (\"Me too! They're the best.\",\n",
       "  \"I have a turtle named Timothy. He's my best friend.\"),\n",
       " ('I love turtles! I used to have a turtle when I was a kid.',\n",
       "  \"That's awesome. What was its name?\"),\n",
       " ('His name was Leonardo. He was a red-eared slider.',\n",
       "  \"I love the name Leonardo. It's so cool.\"),\n",
       " ('Thanks. I thought so too.',\n",
       "  \"I'm glad we met. It's nice to talk to someone who likes dogs and turtles.\"),\n",
       " (\"Me too. It's always nice to meet someone who has similar interests.\",\n",
       "  'I hope we can hang out again sometime.'),\n",
       " (\"I'd love that. We can go play ultimate frisbee or get some ice cream.\",\n",
       "  'That sounds like a plan.'),\n",
       " (\"Great. I'll text you later.\", 'Sounds good. Bye.')]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def conv_to_turns(turns):\n",
    "#     # Split the conversation by newline character\n",
    "#     # turns = conv_lst.split('\\n')\n",
    "    \n",
    "#     # Remove the \"User 1:\" and \"User 2:\" prefixes\n",
    "#     turns = [turn.split(': ', 1)[1] for turn in turns]\n",
    "    \n",
    "#     # Group the turns into tuples\n",
    "#     turns = list(zip(turns[::2], turns[1::2]))\n",
    "    \n",
    "#     return turns\n",
    "\n",
    "# conv_to_turns(conv_lst[0].split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # while\n",
    "# conv_lst = test_set['Best Generated Conversation'].tolist() #.values[0].split(\"\\n\")\n",
    "\n",
    "# eval_turns = []\n",
    "\n",
    "# num_eval_turns = 100\n",
    "\n",
    "# # random.seed(16) # for reproducibility\n",
    "# while True:\n",
    "#     for conv in conv_lst:#[:5]:\n",
    "\n",
    "#         # lazy but I this is fine\n",
    "#         try:\n",
    "#             # split convo str into user/model turns\n",
    "#             turns_raw = conv.split(\"\\n\")\n",
    "#             turns_clean = conv_to_turns(turns_raw)\n",
    "\n",
    "#             # number of turn pairs in the convo\n",
    "#             convo_len = len(turns_clean)\n",
    "#             random_turns = [random.randint(0, convo_len-1) for _ in range(random.randint(0, convo_len-1))] # random number of random pairings of convo turns from the convo \n",
    "#             for i in random_turns:\n",
    "#                 selected_turns = turns_clean[i] # randomly selected turn pairing\n",
    "\n",
    "#                 # these usually mean their are variables to be filled in the convo. Don't want to deal with that\n",
    "#                 ignore = False\n",
    "#                 for char in \"[]\\{\\}\":\n",
    "#                     if char in selected_turns[0] + \" \" + selected_turns[1]:\n",
    "#                         ignore = True\n",
    "\n",
    "#                 # if the turns are valid\n",
    "#                 if selected_turns not in eval_turns:\n",
    "#                     if not ignore:\n",
    "#                         eval_turns.append(selected_turns)\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "#         if len(eval_turns) >= num_eval_turns:\n",
    "#             break\n",
    "\n",
    "# len(eval_turns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
